# -*- coding: utf-8 -*-
"""Scientific_Research_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hBFSjTV-gUNZ6XS0rlJcgcEHnRVp4L_0

Dask loads lager datasets and process the data on personal computers or VM's with limited resources. Dask is a flexible library for parallel computing in Python.
"""

import logging
logger = logging.getLogger("spacy")
logger.setLevel(logging.ERROR)

import dask.bag as db
import json
import pandas as pd

docs = db.read_text('/content/gdrive/MyDrive/Colab Datasets/arxiv-metadata-oai-snapshot.json').map(json.loads)

#Total number of documents
docs.count().compute()

# Taking a sample of one document
docs.take(1)

# The dataset is very huge and we are unaware if the whole dataset could be used. So I have taken the subset of the data for easy access

get_latest_version = lambda x: x['versions'][-1]['created']


# get only the necessary fields of the metadata file
trim = lambda x: {'id': x['id'],
                  'authors': x['authors'],
                  'title': x['title'],
                  'doi': x['doi'],
                  'category':x['categories'].split(' '),
                  'abstract':x['abstract'],}
# filter for papers published on or after 2019-01-01
columns = ['id','category','abstract']
docs_df = (docs.filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2018)
           .map(trim).
           compute())

# convert to pandas
docs_df = pd.DataFrame(docs_df)

#save the trimmed dataset as csv for later use
docs_df.to_csv("trimmed_arxiv_docs.csv", index=False)

# View the first 5 rows
docs_df.head()

"""EDA and some data-cleaning / feature engineering"""

df = pd.read_csv("./trimmed_arxiv_docs.csv")

df.info()

df.shape

# Word count of each abstract
df['abstract_word_count'] = docs_df['abstract'].apply(lambda x: len(x.strip().split()))

df['abstract'].describe(include='all')

# Dropping the duplicated abstracts
df.drop_duplicates(['abstract',], inplace=True)
df['abstract'].describe(include='all')

"""The raw text of the abstracts can't be processed by a model. Hence we transform the data in two methods,

1. Use NLP to restructure the abstract text (ie.) Remove Stop words and punctuation
2. Vectorize the abstact of each paper

# NLP data preprocessing
"""

# the dataframe contains huge data. This affects the data processing. So I am reducing the data size to 10000
df = df.sample(10000, random_state=42)

from tqdm import tqdm
# To download models for the specific purpose to preprocess scientific texts - en_core_sci_lg model can be used
# en_core_sci_lg model is used to preprocess abstracts and it is a model of Spacy
from IPython.utils import io
with io.capture_output() as captured:
    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz

# Import NLP libraries and the spacy package to preprocess the abstract text
import spacy
from spacy.lang.en.stop_words import STOP_WORDS # import common list of stopword
import en_core_sci_lg  # import downloaded model

# Parser
parser = en_core_sci_lg.load()
parser.max_length = 7000000 # Limit the size of the parser

def spacy_tokenizer(sentence):
    ''' Function to preprocess text of scientific papers 
        (e.g Removing Stopword and puntuations)'''
    mytokens = parser(sentence)
    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != "-PRON-" else word.lower_ for word in mytokens ] # Transform to lowercase and split the sentence
    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ] # Remove stopwords and punctuation
    mytokens = " ".join([i for i in mytokens]) 
    return mytokens

import string

punctuations = string.punctuation # List of punctuations to remove from text
stopwords = list(STOP_WORDS)
stopwords[:10]

tqdm.pandas()
df["processed_text"] = df["abstract"].progress_apply(spacy_tokenizer)

"""# Vectorization of the abstracts and dimensionality reduction with PCA"""

# Import vectorizer and define vec function
from sklearn.feature_extraction.text import TfidfVectorizer
def vectorize(text, maxx_features):
    
    vectorizer = TfidfVectorizer(max_features=maxx_features)
    X = vectorizer.fit_transform(text)
    return X

# Vectorize each processed abstract
text = df['processed_text'].values
X = vectorize(text, 2 ** 12) #arbitrary max feature -_> Hyperpara. for optimisation (?)
X.shape

from sklearn.decomposition import PCA

pca = PCA(n_components=0.95, random_state=42) # Taking 95% of the variance
X_reduced= pca.fit_transform(X.toarray())
X_reduced.shape

"""## Clustering using Kmeans"""

from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from scipy.spatial.distance import cdist
from sklearn import metrics

# To find optimal k value
#r_seed = 24
#cluster_errors = []

#for i in range(1, 50):
    #n_clusters = i
    #pipe_pca_kmean = Pipeline([("cluster", KMeans(n_clusters=n_clusters, random_state=r_seed, verbose=0, n_jobs=1))]
    #)

    #pipe_pca_kmean.fit(X_reduced)
    #pipe_pca_kmean.predict(X_reduced)
    #cluster_errors.append(pipe_pca_kmean.named_steps["cluster"].inertia_)

#plt.clf()
#plt.plot(cluster_errors, "o-")
#plt.xlabel("k_clusters")
#plt.ylabel("sum sq distances from mean")
#plt.show()

k = 20 # optimal k found in elbow plot
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X_reduced)
df['kmean_clusters'] = y_pred

"""# t-SNE and umap (Using umap to see the difference from t-SNE)"""

!pip install umap-learn

import umap
from umap import UMAP

# UMAP Definition:
umap_embeddings = UMAP(n_neighbors=100, min_dist=0.3, n_components=2)

X_umap = umap_embeddings.fit_transform(X_reduced)

from sklearn.manifold import TSNE

tsne = TSNE(verbose=1, perplexity=100, random_state=42)
X_embedded = tsne.fit_transform(X.toarray())

"""## Comparing t-SNE and umap"""

import seaborn as sns

# sns settings
sns.set(rc={'figure.figsize':(15,15)})

# colors
palette = sns.color_palette("bright", 1)

# plot
sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], palette=palette)
plt.title('t-SNE without Labels')
plt.savefig("t-sne_arxvid.png")
plt.show()

# sns settings
sns.set(rc={'figure.figsize':(15,15)})

# colors
palette = sns.color_palette("bright", 1)

# plot
sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], palette=palette)
plt.title('umap without Labels')
plt.savefig("umap_arxvid.png")
plt.show()

"""## Plot the Clusters"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# sns settings
sns.set(rc={'figure.figsize':(15,15)})

# colors
palette = sns.hls_palette(20, l=.4, s=.9)

# plot
sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=y_pred, legend='full', palette=palette)
plt.title('t-SNE with Kmeans Labels')
plt.savefig("cluster_tsne.png")
plt.show()

# sns settings
sns.set(rc={'figure.figsize':(15,15)})

# colors
palette = sns.hls_palette(20, l=.4, s=.9)

# plot
sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=y_pred, legend='full', palette=palette)
plt.title('umap with Kmeans Labels')
plt.savefig("cluster_umap_kmeans_labels.png")
plt.show()

"""The labeled plot gives us the insights of how the papers are grouped. It is difficult to say which dimensional reduction performs better for our data. The performance would have to be evaluated in the upcoming steps.

The location of each paper on the plot was determined by umap / t-SNE while the labels (colors) was determined by k-means. If we look at particular parts of the plot where t-SNE and umap have grouped many articles to form a cluster, it is likely that k-means is in uniform labeling of this cluster.

In other cases, the labels (k-means) are more spread out on the plot (umap /t-SNE). This means that umap / t-SNE and k-means have found differences in the higher dimensional data.

This could be because of certain contents in the papers that are overlaping, so it's hard to separate them. This can be observed in the formation of subclusters on the plot.

The algorithms may find connections that were genuine. This may highlight the shared information that are hidden and advance further research.

# Interactive scatter plot based on t-SNE
"""

import plotly.express as px
fig = px.scatter(df, x=X_embedded[:,0], y=X_embedded[:,1], color=y_pred.astype(str),
                 hover_data=['id', 'authors', 'title',],
                 height= 1000, width=1000,
                title = "t-SNE with Kmeans Labels")
fig.show()